{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tt\n",
    "import torch.optim as oo\n",
    "import torch.autograd as ag\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Identity(x): return x\n",
    "class Linear:\n",
    "    r\"\"\" https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def layout(in_features , out_features): \n",
    "        return \\\n",
    "        {\n",
    "            f'weight' : (out_features, in_features),  \n",
    "            f'bias'   : (out_features,            ),  \n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    @tt.no_grad()\n",
    "    def initialize(params:dict, in_features, generator=None, weight='weight', bias='bias'):\n",
    "        k = (1/in_features)**0.5\n",
    "        params[weight].uniform_(-k, k, generator=generator)\n",
    "        params[bias].uniform_(-k, k, generator=generator)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(params, weight, bias, input): return tt.matmul(input, params[weight].T) + params[bias]\n",
    "\n",
    "    @staticmethod\n",
    "    def forwardF(params, weight, bias, actF, input): return actF(tt.matmul(input, params[weight].T) + params[bias])\n",
    "\n",
    "    @staticmethod\n",
    "    def shapes(batch_size, in_features, out_features): \n",
    "        return dict(\n",
    "            in_shape = (batch_size, in_features), \n",
    "            out_shape = (batch_size, out_features),\n",
    "        )\n",
    "\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, input_size, layer_sizes, output_size, activations) -> None:\n",
    "        n_layers = len(layer_sizes)+1\n",
    "        assert len(activations)==n_layers\n",
    "        members = dict(\n",
    "            input_size = input_size,\n",
    "            layer_sizes = layer_sizes,\n",
    "            output_size = output_size,\n",
    "            activations = activations,\n",
    "            n_layers = n_layers,\n",
    "            )\n",
    "        for k,v in members.items(): \n",
    "            if hasattr(self, k):    print(f'[{__class__}] already has attribute [{k}], value [{v}] will be skipped!')\n",
    "            else:                   setattr(self, k, v)\n",
    "\n",
    "    def layout(self):\n",
    "        layer_sizes = [self.input_size] + self.layer_sizes + [self.output_size]\n",
    "        full_layout = {}\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            l = Linear.layout( in_features=layer_sizes[i],  out_features=layer_sizes[i+1],)\n",
    "            full_layout[f'weight_{i}'] = l.pop('weight')\n",
    "            full_layout[f'bias_{i}'] = l.pop('bias')\n",
    "        return full_layout\n",
    "\n",
    "    def forward(self, params, input): \n",
    "        y = input\n",
    "        for i in range(self.n_layers):\n",
    "            y = Linear.forwardF(params, f'weight_{i}', f'bias_{i}', self.activations[i], y)\n",
    "        return y\n",
    "    \n",
    "    def __call__(self, params:dict, *args, **kwargs): return self.forward(params, *args, **kwargs)\n",
    "\n",
    "    def init(self, params, generator=None):\n",
    "        for i in range(self.n_layers): Linear.initialize(params, self.input_size, generator, f'weight_{i}', f'bias_{i}')\n",
    "          \n",
    "    def shapes(self, batch_size):\n",
    "        return Linear.shapes(batch_size, self.input_size, self.output_size)\n",
    "\n",
    "    def params(self, requires_grad=True, dtype=None, device=None) -> dict:\n",
    "        r\"\"\" use to create a new dict of parameters from current layout \"\"\"\n",
    "        return {name : tt.zeros(size=size, dtype=dtype, device=device, requires_grad=requires_grad) for name, size in self.layout().items() }\n",
    "\n",
    "\n",
    "\n",
    "class Modular(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def count(params:dict, requires_grad=None): \n",
    "        r\"\"\" Counts the total number of parameters (numel) in a params\n",
    "        \n",
    "        :param requires_grad: \n",
    "            if None, counts all parameters\n",
    "            if True, counts trainable parameters\n",
    "            if False, counts non-trainiable (frozen) parameters\n",
    "        \"\"\"\n",
    "        return sum( ([ p.numel() for p in params.values() ]) if requires_grad is None else \\\n",
    "                    ([ p.numel() for p in params.values()    if p.requires_grad is requires_grad ]) )\n",
    "\n",
    "    @staticmethod\n",
    "    def show(params:dict, values:bool=False):\n",
    "        r\"\"\" Prints the parameters of a params\n",
    "        \n",
    "        :param values: if True, prints the full tensors otherwise prints only shape\n",
    "        \"\"\"\n",
    "        nos_trainable, nos_frozen = 0, 0\n",
    "        print('=====================================')\n",
    "        for i,(n,p) in enumerate(params.items()):\n",
    "            iparam = p.numel()\n",
    "            if p.requires_grad:\n",
    "                nos_trainable += iparam\n",
    "            else:\n",
    "                nos_frozen += iparam\n",
    "            print(f'#[{i}]\\t{n}\\tShape[{p.shape}]\\tParams: {iparam}\\tTrainable: {p.requires_grad}')\n",
    "            if values: \n",
    "                print('=====================================')\n",
    "                print(f'{p}')\n",
    "                print('=====================================')\n",
    "        print(f'\\nTotal Parameters: {nos_trainable+nos_frozen}\\tTrainable: {nos_trainable}\\tFrozen: {nos_frozen}')\n",
    "        print('=====================================')\n",
    "        return \n",
    "\n",
    "    @staticmethod\n",
    "    @tt.no_grad()\n",
    "    def diff(params1:dict, params2:dict, do_abs:bool=True, do_sum:bool=True):\n",
    "        r\"\"\" Checks the difference between the parameters of two modules.\n",
    "            This can be used to check if two models have exactly the same parameters.\n",
    "\n",
    "        :param do_abs: if True, finds the absolute difference\n",
    "        :param do_sum: if True, finds the sum of difference\n",
    "\n",
    "        :returns: a list of differences in each parameter or their sum if ``do_sum`` is True.\n",
    "        \"\"\"\n",
    "        d = [ (abs(p1 - p2) if do_abs else (p1 - p2)) for p1,p2 in zip(params1.values(), params2.values()) ]\n",
    "        if do_sum: d = [ tt.sum(p) for p in d  ]\n",
    "        return d\n",
    "\n",
    "    @staticmethod\n",
    "    @tt.no_grad()\n",
    "    def copy(params_from:dict, params_to:dict) -> None:\n",
    "        r\"\"\" Copies the parameters of a params to another - both modules are supposed to be identical\"\"\"\n",
    "        for pt,pf in zip(params_to.values(), params_from.values()): pt.copy_(pf)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(params:dict, fn, *args, **kwargs):\n",
    "        r\"\"\" apply a function on parameters and return new set of parameters (NOTE: not using no_grad())\"\"\"\n",
    "        return {k:fn(p, *args, **kwargs) for k,p in params.items()} \n",
    "\n",
    "    @staticmethod\n",
    "    @tt.no_grad()\n",
    "    def apply_(params:dict, fn, *args, **kwargs):\n",
    "        r\"\"\" apply an inplace operation \n",
    "        \n",
    "        NOTE: this can be use to initialize randomly \n",
    "            param.bernoulli_            (p=0.5)                             # p can be tensor a well\n",
    "            param.cauchy_               (median=0.0, sigma=1.0)\n",
    "            param.exponential_          (lambd=1.0)\n",
    "            param.geometric_            (p=0.5)\n",
    "            param.log_normal_           (mean=1.0, std=2.0)\n",
    "            param.normal_               (mean=0.0, std=1.0)\n",
    "            param.random_               (from_=0, to=10)                    # int arguments\n",
    "            param.uniform_              (from_=0.0, to=1.0)\n",
    "        \"\"\"\n",
    "        for p in params.values(): fn(p, *args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    @tt.no_grad()\n",
    "    def clone(params:dict, requires_grad=True, memory_format=None):\n",
    "        r\"\"\" Clones a params by creating copies of parameters using the Tensor.clone() call\n",
    "        No need for .detach() call since we clone with tt.no_grad()\n",
    "        NOTE: this will preserve the ```require_grad``` attribute on all tensors if arg ```requires_grad``` is True \"\"\"\n",
    "        if requires_grad:\n",
    "            return {n:p.clone(memory_format=memory_format).requires_grad_(p.requires_grad) for n,p in params.items()}\n",
    "        else:\n",
    "            return {n:p.clone(memory_format=memory_format).requires_grad_(False) for n,p in params.items()} # force no grad\n",
    "\n",
    "    @staticmethod\n",
    "    @tt.no_grad()\n",
    "    def duplicate(params:dict, copy=True, requires_grad=True, memory_format=None) -> None:\n",
    "        r\"\"\" Duplicates a params by creating copies of parameters using zeros_like() call and copies values if arg  ```copy``` is True\n",
    "        NOTE: this will preserve the ```require_grad``` attribute on all tensors if arg ```requires_grad``` is True \"\"\"\n",
    "        if copy:\n",
    "            if requires_grad:\n",
    "                return {n: tt.zeros_like(p, memory_format=memory_format, requires_grad=p.requires_grad).copy_(p) for n,p in params.items()} \n",
    "            else:\n",
    "                return {n: tt.zeros_like(p, memory_format=memory_format,requires_grad=False).copy_(p) for n,p in params.items()} # force no grad\n",
    "        else:\n",
    "            if requires_grad:\n",
    "                return {n: tt.zeros_like(p, memory_format=memory_format,requires_grad=p.requires_grad) for n,p in params.items()}\n",
    "            else:\n",
    "                return {n: tt.zeros_like(p, memory_format=memory_format,requires_grad=False) for n,p in params.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def replicate(params:dict, n_copies:int=1):\n",
    "        r\"\"\" Replicates a params by storing it in a buffer and retriving many copies\n",
    "        NOTE: this will preserve the ```require_grad``` attribute on all tensors. \"\"\"\n",
    "        from io import BytesIO\n",
    "        buffer = BytesIO()\n",
    "        tt.save(params, buffer)\n",
    "        model_copies = []\n",
    "        for _ in range(n_copies):\n",
    "            buffer.seek(0)\n",
    "            model_copy = tt.load(buffer)\n",
    "            model_copies.append(model_copy)\n",
    "        buffer.close()\n",
    "        del buffer\n",
    "        return model_copy if n_copies==1 else model_copies\n",
    "\n",
    "    @staticmethod\n",
    "    def requires_grad(params:dict, requires:bool, *names):\n",
    "        r\"\"\" Sets requires_grad attribute on tensors in params\n",
    "        if no names are provided, sets requires_grad on all tensors \"\"\"\n",
    "        if names:\n",
    "            for n in names: params[n].requires_grad_(requires)\n",
    "        else: \n",
    "            for p in params.values(): p.requires_grad_(requires)\n",
    "\n",
    "    @staticmethod\n",
    "    def zeros(layout, requires_grad=True, dtype=None, device=None) -> dict:\n",
    "        r\"\"\" use to create a new dict of parameters from given layout::dict[str, size(tuple)] \"\"\"\n",
    "        return {name : tt.zeros(size=size, dtype=dtype, device=device, requires_grad=requires_grad) for name, size in layout.items() }\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros_like(params:dict, memory_format=None) -> dict:\n",
    "        r\"\"\" use to create a new dict of parameters from given layout::dict[str, size(tuple)] \"\"\"\n",
    "        return {name : tt.zeros_like(value, memory_format=memory_format, requires_grad=value.requires_grad) for name, value in params.items() }\n",
    "    \n",
    "    @staticmethod\n",
    "    def save(params:dict, path:str): tt.save(params, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path:str): return tt.load(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "dtype = tt.float32\n",
    "lossF = tt.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(\n",
    "    input_size=input_size, \n",
    "    layer_sizes=[64,64,64],\n",
    "    output_size=output_size,\n",
    "    activations=(tt.sigmoid, tt.tanh, tt.relu, Identity)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 5\n",
    "PHASE, AMP = np.pi, 2.0\n",
    "clients = {}\n",
    "lb, ub, total_size= -5, 5, 100\n",
    "bound_size = int((ub-lb)/num_clients)\n",
    "for i in range(num_clients):\n",
    "    LB= lb + i * bound_size\n",
    "    UB = LB + bound_size\n",
    "    x = tt.linspace(LB, UB, int(total_size/num_clients)).to(dtype=dtype).unsqueeze(-1)\n",
    "    # Y = (tt.sin(X+PHASE)*AMP).to(dtype=dtype)\n",
    "    y = ((tt.tanh(x+PHASE)*tt.tanh(x-PHASE)*AMP)).to(dtype=dtype)\n",
    "\n",
    "    plt.plot(x,y)\n",
    "    clients[f'client_{i}'] = tt.hstack((x,y))\n",
    "    # dataset = tt.hstack((X,Y))\n",
    "    # plt.show()\n",
    "# clients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoch_grad(params, batch):\n",
    "    X, Y = batch[:,0], batch[:,1]\n",
    "    P = model.forward(params, X)\n",
    "    loss = lossF(P, Y)\n",
    "    # print(loss.item())\n",
    "    # lossL.append(loss.item())\n",
    "    grads = ag.grad(loss, params.values())\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "from typing import Tuple, Union\n",
    "from collections import OrderedDict\n",
    "def compute_grad(model: torch.nn.Module,\n",
    "        data_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "        v: Union[Tuple[torch.Tensor, ...], None] = None,\n",
    "        second_order_grads=False, \n",
    "        criterion: Union[torch.nn.CrossEntropyLoss, torch.nn.MSELoss] = None):        \n",
    "        x, y = data_batch\n",
    "        criterion=torch.nn.CrossEntropyLoss()\n",
    "        if second_order_grads:\n",
    "            frz_model_params = deepcopy(model.state_dict())\n",
    "            delta = 1e-3\n",
    "            dummy_model_params_1 = OrderedDict()\n",
    "            dummy_model_params_2 = OrderedDict()\n",
    "            with torch.no_grad():\n",
    "                for (layer_name, param), grad in zip(model.named_parameters(), v):\n",
    "                    dummy_model_params_1.update({layer_name: param + delta * grad})\n",
    "                    dummy_model_params_2.update({layer_name: param - delta * grad})\n",
    "\n",
    "            model.load_state_dict(dummy_model_params_1, strict=False)\n",
    "            logit_1 = model(x)\n",
    "            loss_1 = criterion(logit_1, y)\n",
    "            grads_1 = torch.autograd.grad(loss_1, model.parameters())\n",
    "\n",
    "            model.load_state_dict(dummy_model_params_2, strict=False)\n",
    "            logit_2 = model(x)\n",
    "            loss_2 = criterion(logit_2, y)\n",
    "            grads_2 = torch.autograd.grad(loss_2, model.parameters())\n",
    "\n",
    "            model.load_state_dict(frz_model_params)\n",
    "\n",
    "            grads = []\n",
    "            with torch.no_grad():\n",
    "                for g1, g2 in zip(grads_1, grads_2):\n",
    "                    grads.append((g1 - g2) / (2 * delta))\n",
    "            return grads\n",
    "\n",
    "        else:\n",
    "            logit = model(x)\n",
    "            loss = criterion(logit, y)\n",
    "            grads = torch.autograd.grad(loss, model.parameters())\n",
    "            return grads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per-FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = model.params(requires_grad=True, dtype=dtype)\n",
    "# for k,v in params.items():print(k, v.shape)\n",
    "model.init(params)\n",
    "w_k = params # step 1; w_0\n",
    "r = 1 # fraction of active user; r=1 signifies that all the users are active\n",
    "K = 50 # number of global rounds\n",
    "T = 50\n",
    "alpha = 0.001\n",
    "beta = 0.01\n",
    "batch_size = 8 # independent batches size\n",
    "for k in range (0,K):\n",
    "    for user in clients:\n",
    "        w_i_k_1 = Modular.clone(w_k)\n",
    "        for t in range(0,T):\n",
    "            ds = clients[user]\n",
    "            batch =  next(iter(DataLoader(ds, batch_size=batch_size)))\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
